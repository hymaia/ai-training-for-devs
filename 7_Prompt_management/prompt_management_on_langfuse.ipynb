{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Prompt Management with Langfuse\n",
    "\n",
    "## Why Centralized Prompt Management?\n",
    "\n",
    "Instead of hardcoding prompts in your application, Langfuse lets you:\n",
    "\n",
    "- ‚úÖ **Edit prompts without redeploying** your app\n",
    "- üìä **Version control** your prompts\n",
    "- üîç **Track which prompts** are used in production\n",
    "- üë• **Collaborate** with non-technical team members\n",
    "\n",
    "**Trade-off**: Anyone with access can modify prompts (use versioning wisely!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from langchain.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\n",
    "from langfuse import Langfuse, observe, get_client\n",
    "from qdrant_client import QdrantClient\n",
    "from typing import Optional\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Set your model parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Model Configuration\n",
    "MODEL_NAME = \"gpt-5-nano\"\n",
    "REASONING_EFFORT = \"minimal\"  # \"minimal\" | \"low\" | \"medium\" | \"high\"\n",
    "TEMPERATURE = 0\n",
    "K_RETRIEVAL = 4\n",
    "\n",
    "# üìù Prompt Name (must match your Langfuse prompt name)\n",
    "PROMPT_NAME = \"rag_prompt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Example Prompt Template\n",
    "\n",
    "**‚ö†Ô∏è ACTION REQUIRED**: Copy this template to Langfuse UI:\n",
    "\n",
    "1. Go to [Langfuse Prompts](https://langfuse.com/docs/prompt-management/get-started)\n",
    "2. Create a new prompt named `rag_prompt`\n",
    "3. Paste the template below\n",
    "4. Use variables: `{{question}}`, `{{context}}`, `{{history}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Reference template (to copy to Langfuse UI)\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful assistant answering questions about customer care for AI-Bay.\n",
    "\n",
    "Use the following context documents to answer the user's question. If the answer is not in the provided documents, say \"I don't have that information in the provided documents.\"\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "Conversation History:\n",
    "{history}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer based ONLY on the provided documents\n",
    "2. Be specific and cite which document(s) you used\n",
    "3. If information is unclear or missing, say so\n",
    "4. Keep answers concise but complete\n",
    "5. Use a friendly, informative tone\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# NOTE you have to put double brackets for the variables --> {{question}} instead of {question}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langfuse_client() -> Langfuse:\n",
    "    \"\"\"Initialize Langfuse for observability tracking.\"\"\"\n",
    "    load_dotenv()\n",
    "    return Langfuse(\n",
    "        public_key=os.environ[\"LANGFUSE_PUBLIC_KEY\"],\n",
    "        secret_key=os.environ[\"LANGFUSE_SECRET_KEY\"],\n",
    "        host=os.environ[\"LANGFUSE_BASE_URL\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def load_vector_store(path_to_vector_store: Optional[Path] = None):\n",
    "    \"\"\"Load the vector database containing FAQ documents.\"\"\"\n",
    "    sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "    load_dotenv()\n",
    "\n",
    "    this_dir = Path.cwd().parent\n",
    "    path_to_vector_store = path_to_vector_store or this_dir / \"vector_store\"\n",
    "\n",
    "    print(f\"üìÇ Loading vector store from: {path_to_vector_store}\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    # Remove lock file if exists\n",
    "    lock_file = path_to_vector_store / \".lock\"\n",
    "    if lock_file.exists():\n",
    "        os.remove(lock_file)\n",
    "\n",
    "    client = QdrantClient(path=path_to_vector_store)\n",
    "    return QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=\"faq_collection\",\n",
    "        embedding=embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        vector_name=\"dense\",\n",
    "        sparse_vector_name=\"sparse\",\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "    )\n",
    "\n",
    "\n",
    "def format_docs_alternative(docs: list[Document]):\n",
    "    \"\"\"Format retrieved documents for the prompt.\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        formatted.append(\n",
    "            f\"Document {i}:\\n{doc.metadata['faq_body']}\\nSource: {doc.metadata['faq_id']}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ RAG Conversation Class\n",
    "\n",
    "This class handles:\n",
    "- üîç **Document retrieval** from vector store\n",
    "- üéØ **Prompt fetching** from Langfuse (the magic happens here!)\n",
    "- üí¨ **Response generation** with conversation history\n",
    "- üìä **Automatic tracing** to Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "langfuse = get_client()\n",
    "\n",
    "\n",
    "class RagConversation:\n",
    "    \"\"\"Conversational RAG system with Langfuse observability.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store, llm, history=None):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "        self.history = history if history else []\n",
    "\n",
    "    def add_message(self, message: BaseMessage):\n",
    "        \"\"\"Add a message to conversation history.\"\"\"\n",
    "        self.history.append(message)\n",
    "\n",
    "    @observe(name=\"retriever-call\", as_type=\"retriever\")\n",
    "    def retrieve_documents(self, question, K=K_RETRIEVAL):\n",
    "        \"\"\"üîç Retrieve relevant documents (tracked in Langfuse).\"\"\"\n",
    "        docs_and_scores = self.vector_store.similarity_search_with_relevance_scores(\n",
    "            question, k=K\n",
    "        )\n",
    "        return docs_and_scores\n",
    "\n",
    "    def get_prompt(self, question, context, history):\n",
    "        # TODO: get the prompt from langfuse\n",
    "        # see there https://langfuse.com/docs/prompt-management/get-started\n",
    "        # Do not forget to update the \"prompt\" key of the trace with the langfuse.update_current_generation method see here https://langfuse.com/docs/prompt-management/get-started#link-with-langfuse-tracing-optional\n",
    "        prompt = langfuse.get_prompt(PROMPT_NAME)\n",
    "        # Insert variables into prompt template\n",
    "\n",
    "        compiled_prompt = prompt.compile(\n",
    "            question=question, context=context, history=history\n",
    "        )\n",
    "        langfuse.update_current_generation(\n",
    "            prompt=prompt,\n",
    "        )\n",
    "        return compiled_prompt\n",
    "\n",
    "    @observe()\n",
    "    def history_to_string(self):\n",
    "        \"\"\"Convert conversation history to string format.\"\"\"\n",
    "        langfuse.update_current_trace(metadata={\"history\": self.history})\n",
    "        return \"\\n\".join(\n",
    "            [f\"{message.type}: {message.content}\" for message in self.history]\n",
    "        )\n",
    "\n",
    "    @observe(name=\"llm-call\", as_type=\"generation\")\n",
    "    def generate_response(self, question, docs):\n",
    "        \"\"\"ü§ñ Generate AI response (tracked in Langfuse).\"\"\"\n",
    "        context_str = format_docs_alternative(docs)\n",
    "        prompt = self.get_prompt(question, context_str, self.history_to_string())\n",
    "        response = self.llm.invoke(prompt)\n",
    "\n",
    "        # Update conversation history\n",
    "        self.history.append(HumanMessage(content=question))\n",
    "        self.history.append(AIMessage(content=response.content))\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    @observe\n",
    "    def get_response(self, question):\n",
    "        \"\"\"üî¨ Main method: retrieve docs and generate response.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        docs_and_scores = self.retrieve_documents(question, K=K_RETRIEVAL)\n",
    "        docs = [doc for doc, score in docs_and_scores]\n",
    "        scores = [score for doc, score in docs_and_scores]\n",
    "\n",
    "        # Generate response\n",
    "        response = self.generate_response(question, docs)\n",
    "\n",
    "        return response, docs, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Initialize the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading vector store from: /Users/aniszakari/Documents/ai-training-for-devs/5_Evaluation/vector_store\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# üìÇ Set up paths\n",
    "notebook_dir = Path.cwd()\n",
    "PATH_TO_VECTOR_STORE = notebook_dir.parent / \"5_Evaluation\" / \"vector_store\"\n",
    "\n",
    "vector_store = load_vector_store(path_to_vector_store=PATH_TO_VECTOR_STORE)\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\n",
    "\n",
    "rag_conversation = RagConversation(vector_store, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Interactive Chat Interface\n",
    "\n",
    "**Try it out!** Ask questions about AI-Bay and see:\n",
    "- How prompts are fetched from Langfuse\n",
    "- Traces appearing in the Langfuse UI\n",
    "- Which prompt version was used for each response\n",
    "\n",
    "**Pro tip**: Go to Langfuse ‚Üí Prompts ‚Üí `rag_prompt` to see all traces using this prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_assistant_response(message, history):\n",
    "    \"\"\"Handle chat messages from Gradio interface.\"\"\"\n",
    "    response, docs, scores = rag_conversation.get_response(message)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Launch interactive chat\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_assistant_response,\n",
    "    title=\"ü§ñ AI-Bay Customer Support Assistant\",\n",
    "    description=\"Ask me anything about AI-Bay! Powered by Langfuse prompt management.\",\n",
    "    examples=[\n",
    "        \"How do I post an ad?\",\n",
    "        \"How can I contact a seller?\",\n",
    "        \"Where can I see my messages?\",\n",
    "        \"What are the posting guidelines?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What you learned:\n",
    "\n",
    "1. **Centralized Prompts**: Store prompts in Langfuse, not in code\n",
    "2. **Version Control**: Track which prompt version is used in production\n",
    "3. **No Redeployment**: Update prompts without touching your application\n",
    "4. **Automatic Tracing**: Every prompt usage is logged in Langfuse\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- üîÑ Try modifying the prompt in Langfuse UI and see changes instantly\n",
    "- üìä Explore the Langfuse traces to see prompt performance\n",
    "- üß™ Create multiple prompt versions and A/B test them\n",
    "- üìà Analyze which prompt version performs best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
