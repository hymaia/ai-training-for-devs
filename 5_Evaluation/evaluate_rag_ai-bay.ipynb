{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ RAG System Evaluation for AI-Bay Customer Support\n",
    "\n",
    "This notebook walks you through building and evaluating a **Retrieval-Augmented Generation (RAG)** system for customer support.\n",
    "\n",
    "**What you'll learn:**\n",
    "- üìö How to prepare and chunk FAQ data\n",
    "- üîç Creating a hybrid vector database (dense + sparse)\n",
    "- üí¨ Building a conversational RAG assistant\n",
    "- ‚úÖ Evaluating your RAG system with Langfuse\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Ingestion & Preprocessing üì•\n",
    "\n",
    "First, we'll load the FAQ data and prepare it for our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Load FAQ data\n",
    "notebook_dir = Path.cwd()\n",
    "data_path = notebook_dir / \"data\" / \"faq_en.json\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    faq_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning Functions\n",
    "\n",
    "We need to clean markdown formatting from our FAQ content for better embedding quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_markdown_links_or_images(text: str):\n",
    "    \"\"\"Remove markdown links and images from text.\"\"\"\n",
    "    text = re.sub(r\"!\\[.*?\\]\\(.*?\\)\", \"\", text)  # Remove images\n",
    "    text = re.sub(r\"\\[(.*?)\\]\\(.*?\\)\", r\"\\1\", text)  # Keep link text only\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_asterisks(text: str):\n",
    "    \"\"\"Remove asterisks used for markdown emphasis.\"\"\"\n",
    "    return re.sub(r\"\\*\", \"\", text)\n",
    "\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"Apply all cleaning operations.\"\"\"\n",
    "    text = remove_markdown_links_or_images(text)\n",
    "    text = remove_asterisks(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Document Creation & Chunking ‚úÇÔ∏è\n",
    "\n",
    "Convert FAQs into LangChain documents and split them into optimal chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create documents from FAQ data\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=faq[\"faq_body\"],\n",
    "        metadata={\n",
    "            \"faq_id\": faq[\"faq_id\"],\n",
    "            \"faq_body\": faq[\"faq_body\"],\n",
    "            \"faq_title\": faq[\"faq_title\"],\n",
    "            \"updated_at\": faq[\"updated_at\"],\n",
    "        },\n",
    "    )\n",
    "    for faq in faq_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters.markdown import MarkdownTextSplitter\n",
    "\n",
    "# Chunking configuration\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 70\n",
    "TITLE_KEY = \"faq_title\"\n",
    "\n",
    "\n",
    "def approx_token_length(text: str) -> int:\n",
    "    \"\"\"Estimate token count (rough approximation: 1 token ‚âà 4 chars).\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def add_title_to_chunk(chunk: Document, title_key: str = TITLE_KEY):\n",
    "    \"\"\"Prepend FAQ title to chunk for better context.\"\"\"\n",
    "    chunk.page_content = chunk.metadata[title_key] + \"\\n\\n\" + chunk.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "splitter = MarkdownTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=approx_token_length,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Add titles to chunks\n",
    "for chunk in chunks:\n",
    "    add_title_to_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Vector Store Creation üóÑÔ∏è\n",
    "\n",
    "We'll use **hybrid search** (combining dense vectors + BM25 sparse retrieval) for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create persistent vector store\n",
    "path_to_vector_store = notebook_dir / \"vector_store\"\n",
    "\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    chunks,\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    vector_name=\"dense\",\n",
    "    sparse_vector_name=\"sparse\",\n",
    "    collection_name=\"faq_collection\",\n",
    "    path=path_to_vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: RAG Conversation System üí¨\n",
    "\n",
    "Build a conversational assistant that retrieves relevant FAQs and generates helpful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.base import BaseMessage\n",
    "from langchain.messages import HumanMessage, AIMessage\n",
    "\n",
    "# System prompt for the RAG assistant\n",
    "prompt_template = \"\"\"You are a helpful assistant answering questions about customer care for AI-Bay.\n",
    "\n",
    "Use the following context documents to answer the user's question. If the answer is not in the provided documents, say \"I don't have that information in the provided documents.\"\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "Conversation History:\n",
    "{history}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer based ONLY on the provided documents\n",
    "2. Be specific and cite which document(s) you used\n",
    "3. If information is unclear or missing, say so\n",
    "4. Keep answers concise but complete\n",
    "5. Use a friendly, informative tone\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "def format_docs_alternative(docs):\n",
    "    \"\"\"Format retrieved documents for the prompt.\"\"\"\n",
    "    formatted = [\n",
    "        f\"Document {i}:\\n{doc.metadata['faq_body']}\\nSource: {doc.metadata['faq_id']}\"\n",
    "        for i, doc in enumerate(docs, 1)\n",
    "    ]\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagConversation:\n",
    "    \"\"\"Manages RAG conversation with context retrieval and history.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store, llm, history=None):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "        self.history = history if history else []\n",
    "\n",
    "    def add_message(self, message: BaseMessage):\n",
    "        self.history.append(message)\n",
    "\n",
    "    def history_to_string(self):\n",
    "        return \"\\n\".join(\n",
    "            [f\"{message.type}: {message.content}\" for message in self.history]\n",
    "        )\n",
    "\n",
    "    def get_response(self, question):\n",
    "        # Retrieve relevant context\n",
    "        context = self.vector_store.similarity_search(question, k=4)\n",
    "        context_str = format_docs_alternative(context)\n",
    "\n",
    "        # Create prompt and get response\n",
    "        prompt = prompt_template.format(\n",
    "            context=context_str, history=self.history_to_string(), question=question\n",
    "        )\n",
    "        response = self.llm.invoke(prompt)\n",
    "\n",
    "        # Update history\n",
    "        self.history.append(HumanMessage(content=question))\n",
    "        self.history.append(AIMessage(content=response.content))\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt-5-nano\"\n",
    "REASONING_EFFORT = \"minimal\"  # could be   \"minimal\" | \"low\" | \"medium\" | \"high\"  see [https://platform.openai.com/docs/guides/latest-model]\n",
    "TEMPERATURE = 0\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=TEMPERATURE, reasoning_effort=REASONING_EFFORT\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the RAG System üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation instance\n",
    "rag_conversation = RagConversation(vector_store, llm)\n",
    "\n",
    "# Test with a sample question\n",
    "response = rag_conversation.get_response(\"How do I contact a seller?\")\n",
    "print(\"ü§ñ Assistant:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Interactive Chat Interface üé®\n",
    "\n",
    "Try out your RAG system with a beautiful Gradio interface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create fresh conversation for the UI\n",
    "rag_conversation = RagConversation(vector_store, llm)\n",
    "\n",
    "\n",
    "def rag_assistant_response(message, history):\n",
    "    \"\"\"Handle chat messages from Gradio interface.\"\"\"\n",
    "    return rag_conversation.get_response(message).content\n",
    "\n",
    "\n",
    "# Launch interactive chat\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_assistant_response,\n",
    "    title=\"ü§ñ AI-Bay Customer Support Assistant\",\n",
    "    description=\"Ask me anything about AI-Bay! I'll search our FAQ database to help you.\",\n",
    "    examples=[\n",
    "        \"How do I post an ad?\",\n",
    "        \"How can I contact a seller?\",\n",
    "        \"Where can I see my messages?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluation with Langfuse üìä\n",
    "\n",
    "Now it's time to evaluate how well your RAG system performs!\n",
    "\n",
    "### Step 1: Create Evaluation Dataset\n",
    "\n",
    "Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "uv run python 5_Evaluation/create_langfuse_dataset.py\n",
    "```\n",
    "\n",
    "### Step 2: Run Evaluation\n",
    "\n",
    "Then evaluate your RAG system:\n",
    "\n",
    "```bash\n",
    "uv run python 5_Evaluation/run_evaluation.py\n",
    "```\n",
    "\n",
    "### What Could be Evaluated?\n",
    "\n",
    "- **Retrieval Quality**: Are we finding the right documents?\n",
    "- **Answer Accuracy**: Are responses correct and relevant?\n",
    "- **Hallucinations**: Is the model making things up?\n",
    "- **Response Quality**: Is the answer helpful and well-formatted?\n",
    "\n",
    "Check your Langfuse dashboard to see detailed metrics! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary\n",
    "\n",
    "Congratulations! You've built a complete RAG system:\n",
    "\n",
    "1. ‚úÖ Loaded and preprocessed FAQ data\n",
    "2. ‚úÖ Created a hybrid vector database\n",
    "3. ‚úÖ Built a conversational RAG assistant\n",
    "4. ‚úÖ Tested it with an interactive UI\n",
    "5. ‚úÖ Learned how to evaluate with Langfuse\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different chunk sizes\n",
    "- Try different embedding models\n",
    "- Adjust the number of retrieved documents (k parameter)\n",
    "- Fine-tune the prompt template\n",
    "- Compare retrieval modes (dense vs sparse vs hybrid)\n",
    "\n",
    "Happy experimenting! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
