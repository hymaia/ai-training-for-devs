{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç RAG Chatbot with Langfuse Observability\n",
    "\n",
    "This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG)** chatbot with **built-in observability** using Langfuse.\n",
    "\n",
    "### What you'll learn:\n",
    "- ‚úÖ Build a conversational RAG system\n",
    "- ‚úÖ Track and monitor AI calls in real-time\n",
    "- ‚úÖ Debug your AI application effectively\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Setup & Imports\n",
    "\n",
    "First, let's import all the libraries we need and set up our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from langchain.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\n",
    "from langfuse import Langfuse, observe, get_client\n",
    "from qdrant_client import QdrantClient\n",
    "from typing import Optional\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Configuration\n",
    "\n",
    "Define our model settings and prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Model Configuration\n",
    "MODEL_NAME = \"gpt-5-nano\"\n",
    "REASONING_EFFORT = \"minimal\"  # could be   \"minimal\" | \"low\" | \"medium\" | \"high\"  see [https://platform.openai.com/docs/guides/latest-model]\n",
    "TEMPERATURE = 0\n",
    "K_RETRIEVAL = 4\n",
    "\n",
    "# üìù System Prompt Template\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful assistant answering questions about customer care for AI-Bay.\n",
    "\n",
    "Use the following context documents to answer the user's question. If the answer is not in the provided documents, say \"I don't have that information in the provided documents.\"\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "here is the history of the conversation:\n",
    "{history}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer based ONLY on the provided documents\n",
    "2. Be specific and cite which document(s) you used\n",
    "3. If information is unclear or missing, say so\n",
    "4. Keep answers concise but complete\n",
    "5. Use a friendly, informative tone\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 3: Helper Functions\n",
    "\n",
    "These functions handle Langfuse connection, vector store loading, and document formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langfuse_client() -> Langfuse:\n",
    "    \"\"\"Initialize Langfuse for observability tracking.\"\"\"\n",
    "    load_dotenv()\n",
    "    return Langfuse(\n",
    "        public_key=os.environ[\"LANGFUSE_PUBLIC_KEY\"],\n",
    "        secret_key=os.environ[\"LANGFUSE_SECRET_KEY\"],\n",
    "        host=os.environ[\"LANGFUSE_BASE_URL\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def load_vector_store(path_to_vector_store: Optional[Path] = None):\n",
    "    \"\"\"Load the vector database containing FAQ documents.\"\"\"\n",
    "    sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "    load_dotenv()\n",
    "\n",
    "    this_dir = Path.cwd().parent\n",
    "    path_to_vector_store = path_to_vector_store or this_dir / \"vector_store\"\n",
    "\n",
    "    print(f\"üìÇ Loading vector store from: {path_to_vector_store}\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    # Remove lock file if exists\n",
    "    lock_file = path_to_vector_store / \".lock\"\n",
    "    if lock_file.exists():\n",
    "        os.remove(lock_file)\n",
    "\n",
    "    client = QdrantClient(path=path_to_vector_store)\n",
    "    return QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=\"faq_collection\",\n",
    "        embedding=embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        vector_name=\"dense\",\n",
    "        sparse_vector_name=\"sparse\",\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "    )\n",
    "\n",
    "\n",
    "def format_docs_alternative(docs: list[Document]):\n",
    "    \"\"\"Format retrieved documents for the prompt.\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        formatted.append(\n",
    "            f\"Document {i}:\\n{doc.metadata['faq_body']}\\nSource: {doc.metadata['faq_id']}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4: RAG Conversation Class\n",
    "\n",
    "This is the heart of our chatbot! The `@observe` decorators enable **automatic tracking** in Langfuse.\n",
    "\n",
    "### Key Features:\n",
    "- üîç **Document Retrieval**: Finds relevant FAQs\n",
    "- üí¨ **Response Generation**: Creates AI responses\n",
    "- üìä **Auto-Logging**: Tracks everything in Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "langfuse = get_client()\n",
    "\n",
    "\n",
    "class RagConversation:\n",
    "    \"\"\"Conversational RAG system with Langfuse observability.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store, llm, history=None):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "        self.history = history if history else []\n",
    "\n",
    "    def add_message(self, message: BaseMessage):\n",
    "        \"\"\"Add a message to conversation history.\"\"\"\n",
    "        self.history.append(message)\n",
    "\n",
    "    @observe(name=\"retriever-call\", as_type=\"retriever\")\n",
    "    def retrieve_documents(self, question, K=K_RETRIEVAL):\n",
    "        \"\"\"üîç Retrieve relevant documents (tracked in Langfuse).\"\"\"\n",
    "        docs_and_scores = self.vector_store.similarity_search_with_relevance_scores(\n",
    "            question, k=K\n",
    "        )\n",
    "        return docs_and_scores\n",
    "\n",
    "    @observe()\n",
    "    def history_to_string(self):\n",
    "        \"\"\"Convert conversation history to string format.\"\"\"\n",
    "        # Notice how we can dynamically update the trace metadata and add the stuff we want to track\n",
    "        # see https://langfuse.com/faq/all/empty-trace-input-and-output\n",
    "        # because we don't return something explicitly we have to manually update the trace if we want to keep track of it\n",
    "        langfuse.update_current_trace(metadata={\"history\": self.history})\n",
    "        return \"\\n\".join(\n",
    "            [f\"{message.type}: {message.content}\" for message in self.history]\n",
    "        )\n",
    "\n",
    "    @observe(name=\"llm-call\", as_type=\"generation\")\n",
    "    def generate_response(self, question, docs):\n",
    "        \"\"\"ü§ñ Generate AI response (tracked in Langfuse).\"\"\"\n",
    "        context_str = format_docs_alternative(docs)\n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            context=context_str, question=question, history=self.history_to_string()\n",
    "        )\n",
    "        response = self.llm.invoke(prompt)\n",
    "\n",
    "        # Update conversation history\n",
    "        self.history.append(HumanMessage(content=question))\n",
    "        self.history.append(AIMessage(content=response.content))\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    @observe\n",
    "    def get_response(self, question):\n",
    "        \"\"\"üì¨ Main method: retrieve docs and generate response.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        docs_and_scores = self.retrieve_documents(question, K=K_RETRIEVAL)\n",
    "        docs = [doc for doc, score in docs_and_scores]\n",
    "        scores = [score for doc, score in docs_and_scores]\n",
    "\n",
    "        # Generate response\n",
    "        response = self.generate_response(question, docs)\n",
    "\n",
    "        return response, docs, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Initialize the System\n",
    "\n",
    "Load the vector store and initialize the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# üìç Set up paths\n",
    "notebook_dir = Path.cwd()\n",
    "PATH_TO_VECTOR_STORE = notebook_dir.parent / \"5_Evaluation\" / \"vector_store\"\n",
    "\n",
    "# üìö Load vector store\n",
    "vector_store = load_vector_store(path_to_vector_store=PATH_TO_VECTOR_STORE)\n",
    "\n",
    "# üß† Initialize LLM\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\n",
    "\n",
    "# üí¨ Create conversation instance\n",
    "rag_conversation = RagConversation(vector_store, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 6: Launch the Chat Interface\n",
    "\n",
    "### üéâ Try it out!\n",
    "\n",
    "Once you run this cell, a **Gradio interface** will appear below. You can:\n",
    "- Ask questions about AI-Bay\n",
    "- View responses in real-time\n",
    "- Check **Langfuse dashboard** to see all tracked calls!\n",
    "\n",
    "**Tip**: Open your Langfuse dashboard in another tab to watch the magic happen! ü™Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def rag_assistant_response(message, history):\n",
    "    \"\"\"Handle chat messages from Gradio interface.\"\"\"\n",
    "    response, docs, scores = rag_conversation.get_response(message)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Launch interactive chat\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_assistant_response,\n",
    "    title=\"ü§ñ AI-Bay Customer Support Assistant\",\n",
    "    description=\"Ask me anything about AI-Bay! I'll search our FAQ database to help you.\",\n",
    "    examples=[\n",
    "        \"How do I post an ad?\",\n",
    "        \"How can I contact a seller?\",\n",
    "        \"Where can I see my messages?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What's Happening Behind the Scenes?\n",
    "\n",
    "Every time you ask a question:\n",
    "\n",
    "1. **üîç Retrieval**: The system searches the vector database for relevant FAQs\n",
    "2. **üìù Context Building**: Selected documents are formatted into the prompt\n",
    "3. **ü§ñ Generation**: The LLM generates a response based on the context\n",
    "4. **üìä Logging**: All steps are automatically logged to Langfuse\n",
    "\n",
    "### üî¨ Check Langfuse to see:\n",
    "- Response times for each step\n",
    "- Token usage\n",
    "- Retrieved documents and their scores\n",
    "- Full conversation history\n",
    "- Cost tracking\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "Try experimenting with:\n",
    "- Different `K_RETRIEVAL` values (2-10)\n",
    "- Different `TEMPERATURE` settings (0-1)\n",
    "- Different models (gpt-4, gpt-4-turbo, etc.)\n",
    "- Modifying the `PROMPT_TEMPLATE`\n",
    "\n",
    "**Happy Learning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
