{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ü§ñ Using LLMs with LangChain - A Beginner's Guide\n",
    "\n",
    "Welcome to this interactive tutorial! In this notebook, you'll learn how to:\n",
    "- üîß Set up and configure chat models\n",
    "- üí¨ Interact with AI using different message types\n",
    "- üöÄ Make your first API calls to language models\n",
    "\n",
    "Let's dive in! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö What are Chat Models?\n",
    "\n",
    "Chat models are AI systems that:\n",
    "- Take a **sequence of messages** as input\n",
    "- Return **intelligent responses** as output\n",
    "\n",
    "Think of it like having a conversation with an AI assistant!\n",
    "\n",
    "### üåü Why LangChain?\n",
    "\n",
    "LangChain makes it easy to work with different AI models through a unified interface. In this course, we'll use:\n",
    "\n",
    "| Model | Why? | Link |\n",
    "|-------|------|------|\n",
    "| **ChatOpenAI** | Popular & performant | [Documentation](https://docs.langchain.com/oss/python/integrations/chat/openai) |\n",
    "\n",
    "> **üìù Note:** You'll need an `OPENAI_API_KEY` to follow along. Don't have one? Check out [OpenAI's platform](https://platform.openai.com/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## üîë Step 1: Environment Setup\n",
    "\n",
    "First, let's load our API keys from the environment. This keeps your secrets safe! üîí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-setup",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéõÔ∏è Step 2: Initialize the Chat Model\n",
    "\n",
    "Now we'll set up our AI model. We have two options:\n",
    "\n",
    "### Option A: Using OpenRouter üîÑ\n",
    "\n",
    "OpenRouter allows you to access multiple AI models through a single API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "openrouter-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Configure model with OpenRouter\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",  # Specify the model you want to use\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0,  # 0 = deterministic, 1 = creative\n",
    ")\n",
    "\n",
    "print(\"üéâ Model initialized with OpenRouter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-openai",
   "metadata": {},
   "source": [
    "### Option B: Direct OpenAI Connection üéØ\n",
    "\n",
    "Or connect directly to OpenAI's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-openai-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Simple direct connection\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "print(\"üéâ Model initialized with OpenAI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temperature-explained",
   "metadata": {},
   "source": [
    "#### üå°Ô∏è Understanding Temperature\n",
    "\n",
    "The `temperature` parameter controls response creativity:\n",
    "\n",
    "```\n",
    "0.0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1.0\n",
    "‚îÇ                            ‚îÇ\n",
    "Predictable              Creative\n",
    "Consistent               Varied\n",
    "Focused                  Exploratory\n",
    "```\n",
    "\n",
    "- **temperature=0**: Best for factual answers, code, translations\n",
    "- **temperature=0.7**: Good balance for most tasks\n",
    "- **temperature=1**: Maximum creativity for stories, brainstorming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-interaction",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Your First AI Interaction!\n",
    "\n",
    "Let's send a message to the AI and see what happens! üéä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-invoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending a simple string message\n",
    "response = model.invoke(\"Hello, world!\")\n",
    "\n",
    "print(\"üì© Full Response Object:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "response-structure",
   "metadata": {},
   "source": [
    "### üîç Understanding the Response\n",
    "\n",
    "The response object contains:\n",
    "- `content`: The actual AI message\n",
    "- `response_metadata`: Token usage, costs, model info\n",
    "- `id`: Unique identifier for this interaction\n",
    "\n",
    "Most of the time, you'll just want the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the message content\n",
    "print(\"üí¨ AI says:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "messages-explanation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí¨ Step 4: Working with Message Roles\n",
    "\n",
    "In real conversations, different participants have different roles. In AI chat, we typically have:\n",
    "\n",
    "| Role | Purpose | Example |\n",
    "|------|---------|------|\n",
    "| **System** üé≠ | Sets behavior/personality | \"You are a helpful coding assistant\" |\n",
    "| **User** üë§ | Your questions/inputs | \"What is the capital of France?\" |\n",
    "| **Assistant** ü§ñ | AI's responses | \"The capital of France is Paris.\" |\n",
    "\n",
    "![Message Flow](https://via.placeholder.com/700x200/9B59B6/FFFFFF?text=System+Sets+Rules+‚Üí+User+Asks+‚Üí+Assistant+Answers)\n",
    "\n",
    "### üéØ System Messages: Setting the Stage\n",
    "\n",
    "System messages are like giving the AI a role to play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message-roles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a structured conversation\n",
    "response = model.invoke(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful geography expert who loves to share interesting facts.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üåç Geography Expert says:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "message-types",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì® Step 5: Using Message Objects\n",
    "\n",
    "LangChain provides special message classes for cleaner code:\n",
    "\n",
    "```python\n",
    "HumanMessage    ‚Üí Messages from the user\n",
    "AIMessage       ‚Üí Responses from the AI  \n",
    "SystemMessage   ‚Üí System instructions\n",
    "```\n",
    "\n",
    "### üé® Benefits of Message Objects:\n",
    "- üìù More readable code\n",
    "- üè∑Ô∏è Can add metadata (like names)\n",
    "- üîÑ Easier to build conversation histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message-objects",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message with metadata\n",
    "msg = HumanMessage(\n",
    "    content=\"Hello world! I'm excited to learn about AI!\",\n",
    "    name=\"Student\",  # Optional: identify the speaker\n",
    ")\n",
    "\n",
    "# Send as a list\n",
    "messages = [msg]\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"üéì AI's response to student:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3607efc",
   "metadata": {},
   "source": [
    "The response contains multiple pieces of information:\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content='...',              # The actual message text\n",
    "    response_metadata={         # Metadata about the response\n",
    "        'token_usage': {...},   # How many tokens were used\n",
    "        'model_name': '...',    # Which model processed this\n",
    "        'finish_reason': '...'  # Why the response ended\n",
    "    },\n",
    "    id='...',                   # Unique identifier\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shorthand",
   "metadata": {},
   "source": [
    "### ‚ö° Quick Tip: Shorthand Method\n",
    "\n",
    "For simple queries, you can skip the message objects entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shorthand-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This automatically converts to a HumanMessage\n",
    "response = model.invoke(\"What's the weather like?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-overview",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Step 6: Essential Methods\n",
    "\n",
    "LangChain chat models come with powerful methods:\n",
    "\n",
    " üìû `invoke()` - Single Response\n",
    "```python\n",
    "response = model.invoke(\"Your question\")\n",
    "# Returns complete response at once\n",
    "```\n",
    "\n",
    "**Best for:** Short answers, quick questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38778099",
   "metadata": {},
   "source": [
    "üåä `stream()` - Real-time Streaming\n",
    "```python\n",
    "for chunk in model.stream(\"Tell me a story\"):\n",
    "    print(chunk.content, end=\"\")\n",
    "# Shows response as it's generated\n",
    "```\n",
    "\n",
    "**Best for:** Long responses, better UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"Tell me a story\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb9fa2",
   "metadata": {},
   "source": [
    "Sometimes you might want to use the builtin rompttemplate and the chains of LangChains (But I don't really like them :p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30126f7",
   "metadata": {},
   "source": [
    "You can use tuple for messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ca810",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a viral twitter influencer grading a tweet. Generate critique and recommendations for the user's tweet.\"\n",
    "            \"Always provide detailed recommendations, including requests for length, virality, style, etc.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe25257",
   "metadata": {},
   "source": [
    "Or you can use dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a twitter techie influencer assistant tasked with writing excellent twitter posts.\"\n",
    "            \" Generate the best twitter post possible for the user's request.\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want to write a twitter post about the latest trends in AI.\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using message objects with metadata\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful coding tutor specializing in Python.\"),\n",
    "    HumanMessage(\n",
    "        content=\"How do I reverse a string in Python?\",\n",
    "        name=\"Student\",  # Optional: identify the speaker\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"üéì Tutor's Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87385634",
   "metadata": {},
   "source": [
    "### Building Conversation History üí≠\n",
    "\n",
    "For multi-turn conversations, include previous messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44471673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a multi-turn conversation\n",
    "conversation = [\n",
    "    SystemMessage(content=\"You are a math tutor.\"),\n",
    "    HumanMessage(content=\"What is 15 * 24?\"),\n",
    "    AIMessage(content=\"15 * 24 = 360\"),\n",
    "    HumanMessage(content=\"Can you show me how you calculated that?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "\n",
    "print(\"üìö Math Tutor:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1f80b",
   "metadata": {},
   "source": [
    "\n",
    "## LangChain Expression Language (LCEL) üîó\n",
    "\n",
    "### What is LCEL? ü§î\n",
    "\n",
    "LCEL is LangChain's way of chaining components together using the `|` (pipe) operator.\n",
    "\n",
    "```python\n",
    "# Traditional approach\n",
    "formatted = prompt.format(...)\n",
    "response = model.invoke(formatted)\n",
    "\n",
    "# LCEL approach ‚ú®\n",
    "chain = prompt | model\n",
    "response = chain.invoke({...})\n",
    "```\n",
    "\n",
    "### Why Use LCEL? üåü\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Composability** | Chain multiple operations easily |\n",
    "| **Streaming** | Automatic streaming support |\n",
    "| **Async** | Built-in async execution |\n",
    "| **Batching** | Process multiple inputs efficiently |\n",
    "| **Debugging** | Better error messages and tracing |\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "Input ‚îÄ‚îÄ> Prompt ‚îÄ‚îÄ> Model ‚îÄ‚îÄ> Output Parser ‚îÄ‚îÄ> Final Result\n",
    "          Template\n",
    "          \n",
    "With LCEL:\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"o4-mini\")\n",
    "\n",
    "generate_chain = generation_prompt | llm\n",
    "reflect_chain = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429529aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"I want to write a twitter post about the latest trends in AI.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccad160",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflect_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"I want to write a twitter post about the latest trends in AI.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "Congratulations! üéâ You've learned:\n",
    "\n",
    "‚úÖ How to set up LangChain chat models  \n",
    "‚úÖ The difference between message roles  \n",
    "‚úÖ How to invoke models with strings and message objects  \n",
    "‚úÖ Understanding response objects and extracting content  \n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- Learn about prompt templates\n",
    "- Build conversation chains\n",
    "- Explore memory and context\n",
    "- Create RAG agents\n",
    "\n",
    "### üìö Resources:\n",
    "\n",
    "- [LangChain Documentation](https://docs.langchain.com)\n",
    "- [LangChain Python Reference](https://reference.langchain.com/python)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding!** üöÄ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
