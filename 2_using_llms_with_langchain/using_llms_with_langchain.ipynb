{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¤– Using LLMs with LangChain - A Beginner's Guide\n",
    "\n",
    "Welcome to this interactive tutorial! In this notebook, you'll learn how to:\n",
    "- ğŸ”§ Set up and configure chat models\n",
    "- ğŸ’¬ Interact with AI using different message types\n",
    "- ğŸš€ Make your first API calls to language models\n",
    "\n",
    "Let's dive in! ğŸ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š What are Chat Models?\n",
    "\n",
    "Chat models are AI systems that:\n",
    "- Take a **sequence of messages** as input\n",
    "- Return **intelligent responses** as output\n",
    "\n",
    "Think of it like having a conversation with an AI assistant!\n",
    "\n",
    "### ğŸŒŸ Why LangChain?\n",
    "\n",
    "LangChain makes it easy to work with different AI models through a unified interface. In this course, we'll use:\n",
    "\n",
    "| Model | Why? | Link |\n",
    "|-------|------|------|\n",
    "| **ChatOpenAI** | Popular & performant | [Documentation](https://docs.langchain.com/oss/python/integrations/chat/openai) |\n",
    "\n",
    "> **ğŸ“ Note:** You'll need an `OPENAI_API_KEY` to follow along. Don't have one? Check out [OpenAI's platform](https://platform.openai.com/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Step 1: Environment Setup\n",
    "\n",
    "First, let's load our API keys from the environment. This keeps your secrets safe! ğŸ”’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-setup",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ›ï¸ Step 2: Initialize the Chat Model\n",
    "\n",
    "Now we'll set up our AI model. We have two options:\n",
    "\n",
    "### Option A: Using OpenRouter ğŸ”„\n",
    "\n",
    "OpenRouter allows you to access multiple AI models through a single API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "openrouter-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ Model initialized with OpenRouter!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Configure model with OpenRouter\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",  # Specify the model you want to use\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0,  # 0 = deterministic, 1 = creative\n",
    ")\n",
    "\n",
    "print(\"ğŸ‰ Model initialized with OpenRouter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-openai",
   "metadata": {},
   "source": [
    "### Option B: Direct OpenAI Connection ğŸ¯\n",
    "\n",
    "Or connect directly to OpenAI's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "direct-openai-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ Model initialized with OpenAI!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Simple direct connection\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "print(\"ğŸ‰ Model initialized with OpenAI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temperature-explained",
   "metadata": {},
   "source": [
    "#### ğŸŒ¡ï¸ Understanding Temperature\n",
    "\n",
    "The `temperature` parameter controls response creativity:\n",
    "\n",
    "```\n",
    "0.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.0\n",
    "â”‚                            â”‚\n",
    "Predictable              Creative\n",
    "Consistent               Varied\n",
    "Focused                  Exploratory\n",
    "```\n",
    "\n",
    "- **temperature=0**: Best for factual answers, code, translations\n",
    "- **temperature=0.7**: Good balance for most tasks\n",
    "- **temperature=1**: Maximum creativity for stories, brainstorming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-interaction",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 3: Your First AI Interaction!\n",
    "\n",
    "Let's send a message to the AI and see what happens! ğŸŠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simple-invoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“© Full Response Object:\n",
      "==================================================\n",
      "content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_7abc656409', 'id': 'chatcmpl-Clcpi1RANQYky4WDzB1teNH9wHlLM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b0e04-5ca4-7ca2-af90-d66d5020dc7b-0' usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Sending a simple string message\n",
    "response = model.invoke(\"Hello, world!\")\n",
    "\n",
    "print(\"ğŸ“© Full Response Object:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "response-structure",
   "metadata": {},
   "source": [
    "### ğŸ” Understanding the Response\n",
    "\n",
    "The response object contains:\n",
    "- `content`: The actual AI message\n",
    "- `response_metadata`: Token usage, costs, model info\n",
    "- `id`: Unique identifier for this interaction\n",
    "\n",
    "Most of the time, you'll just want the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extract-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ AI says:\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Extract just the message content\n",
    "print(\"ğŸ’¬ AI says:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "messages-explanation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¬ Step 4: Working with Message Roles\n",
    "\n",
    "In real conversations, different participants have different roles. In AI chat, we typically have:\n",
    "\n",
    "| Role | Purpose | Example |\n",
    "|------|---------|------|\n",
    "| **System** ğŸ­ | Sets behavior/personality | \"You are a helpful coding assistant\" |\n",
    "| **User** ğŸ‘¤ | Your questions/inputs | \"What is the capital of France?\" |\n",
    "| **Assistant** ğŸ¤– | AI's responses | \"The capital of France is Paris.\" |\n",
    "\n",
    "![Message Flow](https://via.placeholder.com/700x200/9B59B6/FFFFFF?text=System+Sets+Rules+â†’+User+Asks+â†’+Assistant+Answers)\n",
    "\n",
    "### ğŸ¯ System Messages: Setting the Stage\n",
    "\n",
    "System messages are like giving the AI a role to play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "message-roles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Geography Expert says:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Creating a structured conversation\n",
    "response = model.invoke(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful geography expert who loves to share interesting facts.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ğŸŒ Geography Expert says:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "message-types",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“¨ Step 5: Using Message Objects\n",
    "\n",
    "LangChain provides special message classes for cleaner code:\n",
    "\n",
    "```python\n",
    "HumanMessage    â†’ Messages from the user\n",
    "AIMessage       â†’ Responses from the AI  \n",
    "SystemMessage   â†’ System instructions\n",
    "```\n",
    "\n",
    "### ğŸ¨ Benefits of Message Objects:\n",
    "- ğŸ“ More readable code\n",
    "- ğŸ·ï¸ Can add metadata (like names)\n",
    "- ğŸ”„ Easier to build conversation histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message-objects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ AI's response to student:\n",
      "Hello! That's awesome to hear you're excited about AI! How can I help you get started? Are you interested in learning about how AI works, different types of AI, or maybe some hands-on projects?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Create a message with metadata\n",
    "msg = HumanMessage(\n",
    "    content=\"Hello world! I'm excited to learn about AI!\",\n",
    "    name=\"Student\",  # Optional: identify the speaker\n",
    ")\n",
    "\n",
    "# Send as a list\n",
    "messages = [msg]\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"ğŸ“ AI's response to student:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3607efc",
   "metadata": {},
   "source": [
    "The response contains multiple pieces of information:\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content='...',              # The actual message text\n",
    "    response_metadata={         # Metadata about the response\n",
    "        'token_usage': {...},   # How many tokens were used\n",
    "        'model_name': '...',    # Which model processed this\n",
    "        'finish_reason': '...'  # Why the response ended\n",
    "    },\n",
    "    id='...',                   # Unique identifier\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shorthand",
   "metadata": {},
   "source": [
    "### âš¡ Quick Tip: Shorthand Method\n",
    "\n",
    "For simple queries, you can skip the message objects entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shorthand-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have access to real-time weather data. You can check the current weather by using a weather website or app like Weather.com, AccuWeather, or a voice assistant on your device. If you tell me your location, I can help guide you on where to find the forecast!\n"
     ]
    }
   ],
   "source": [
    "# This automatically converts to a HumanMessage\n",
    "response = model.invoke(\"What's the weather like?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-overview",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ› ï¸ Step 6: Essential Methods\n",
    "\n",
    "LangChain chat models come with powerful methods:\n",
    "\n",
    " ğŸ“ `invoke()` - Single Response\n",
    "```python\n",
    "response = model.invoke(\"Your question\")\n",
    "# Returns complete response at once\n",
    "```\n",
    "\n",
    "**Best for:** Short answers, quick questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38778099",
   "metadata": {},
   "source": [
    "ğŸŒŠ `stream()` - Real-time Streaming\n",
    "```python\n",
    "for chunk in model.stream(\"Tell me a story\"):\n",
    "    print(chunk.content, end=\"\")\n",
    "# Shows response as it's generated\n",
    "```\n",
    "\n",
    "**Best for:** Long responses, better UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb3093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Hereâ€™s a story for you:\n",
      "\n",
      "---\n",
      "\n",
      "**The Lantern of Whispering Woods**\n",
      "\n",
      "In a small village nestled at the edge of the Whispering Woods, there lived a curious girl named Elara. The villagers often spoke of the forest with a mix of fear and wonder, for it was said that the trees could whisper secrets to those who dared to listen.\n",
      "\n",
      "One evening, as the sun dipped below the horizon, Elara found an old, dusty lantern in her grandmotherâ€™s attic. It was unlike any lantern she had seen beforeâ€”its glass glowed faintly with a soft, blue light, and intricate symbols were etched into its metal frame.\n",
      "\n",
      "Intrigued, Elara took the lantern and ventured into the Whispering Woods. As she walked deeper among the ancient trees, the whispers grew louder, forming words she could almost understand. The lanternâ€™s light pulsed gently, guiding her path.\n",
      "\n",
      "Suddenly, she came upon a clearing where the air shimmered with magic. In the center stood a majestic tree, its bark glowing with the same blue light as the lantern. Elara raised the lantern, and the treeâ€™s whispers became clear: it spoke of a hidden realm where lost dreams and forgotten hopes lived, waiting to be found.\n",
      "\n",
      "With a brave heart, Elara touched the tree, and a doorway of light opened before her. Stepping through, she found herself in a world filled with vibrant colors and shimmering stars, where every dream she had ever wished for danced around her.\n",
      "\n",
      "Elara realized that the lantern was a keyâ€”not just to the forestâ€™s secrets, but to the magic within herself. She returned to her village, carrying the light of the Whispering Woods in her heart, ready to share its wonder with those who dared to dream.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to tell you another story?"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"Tell me a story\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb9fa2",
   "metadata": {},
   "source": [
    "Sometimes you might want to use the builtin rompttemplate and the chains of LangChains (But I don't really like them :p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e09f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30126f7",
   "metadata": {},
   "source": [
    "You can use tuple for messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf6ca810",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a viral twitter influencer grading a tweet. Generate critique and recommendations for the user's tweet.\"\n",
    "            \"Always provide detailed recommendations, including requests for length, virality, style, etc.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe25257",
   "metadata": {},
   "source": [
    "Or you can use dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "410d8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a twitter techie influencer assistant tasked with writing excellent twitter posts.\"\n",
    "            \" Generate the best twitter post possible for the user's request.\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want to write a twitter post about the latest trends in AI.\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4532ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Tutor's Response:\n",
      "You can reverse a string in Python using slicing. Here's a simple example:\n",
      "\n",
      "```python\n",
      "my_string = \"Hello, world!\"\n",
      "reversed_string = my_string[::-1]\n",
      "print(reversed_string)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "!dlrow ,olleH\n",
      "```\n",
      "\n",
      "The slice `[::-1]` means start from the end towards the first taking each character in reverse order. Let me know if you'd like other methods!\n"
     ]
    }
   ],
   "source": [
    "# Using message objects with metadata\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful coding tutor specializing in Python.\"),\n",
    "    HumanMessage(\n",
    "        content=\"How do I reverse a string in Python?\",\n",
    "        name=\"Student\",  # Optional: identify the speaker\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"ğŸ“ Tutor's Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87385634",
   "metadata": {},
   "source": [
    "### Building Conversation History ğŸ’­\n",
    "\n",
    "For multi-turn conversations, include previous messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44471673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a multi-turn conversation\n",
    "conversation = [\n",
    "    SystemMessage(content=\"You are a math tutor.\"),\n",
    "    HumanMessage(content=\"What is 15 * 24?\"),\n",
    "    AIMessage(content=\"15 * 24 = 360\"),\n",
    "    HumanMessage(content=\"Can you show me how you calculated that?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "\n",
    "print(\"ğŸ“š Math Tutor:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1f80b",
   "metadata": {},
   "source": [
    "\n",
    "## LangChain Expression Language (LCEL) ğŸ”—\n",
    "\n",
    "### What is LCEL? ğŸ¤”\n",
    "\n",
    "LCEL is LangChain's way of chaining components together using the `|` (pipe) operator.\n",
    "\n",
    "```python\n",
    "# Traditional approach\n",
    "formatted = prompt.format(...)\n",
    "response = model.invoke(formatted)\n",
    "\n",
    "# LCEL approach âœ¨\n",
    "chain = prompt | model\n",
    "response = chain.invoke({...})\n",
    "```\n",
    "\n",
    "### Why Use LCEL? ğŸŒŸ\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Composability** | Chain multiple operations easily |\n",
    "| **Streaming** | Automatic streaming support |\n",
    "| **Async** | Built-in async execution |\n",
    "| **Batching** | Process multiple inputs efficiently |\n",
    "| **Debugging** | Better error messages and tracing |\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "Input â”€â”€> Prompt â”€â”€> Model â”€â”€> Output Parser â”€â”€> Final Result\n",
    "          Template\n",
    "          \n",
    "With LCEL:\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3623a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebbd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"o4-mini\")\n",
    "\n",
    "generate_chain = generation_prompt | llm\n",
    "reflect_chain = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "429529aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hot AI trends to watch in 2024:  \\nâ€¢ Generative & multimodal foundation models  \\nâ€¢ Autonomous AI agents & workflows  \\nâ€¢ Self-supervised & federated learning for privacy  \\nâ€¢ TinyML bringing intelligence to the edge  \\nâ€¢ XAI & Responsible AI for trust  \\n  \\nThe future is nowâ€”are you ready? ğŸ¤–âœ¨ #AI #MachineLearning #TechNews', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 479, 'prompt_tokens': 66, 'total_tokens': 545, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-Clcu0vZXhbuT3R3wOFmLLFXzONnwo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0e08-6b82-7b62-bd28-b4eb4bc2c3da-0', usage_metadata={'input_tokens': 66, 'output_tokens': 479, 'total_tokens': 545, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"I want to write a twitter post about the latest trends in AI.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ccad160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hereâ€™s a blueprint for a scroll-stopping AI-trends tweet, plus a breakdown of what works, what can be tightened, and how to boost reach and engagement.\\n\\n1) Sample Tweet (â‰ˆ195 characters)  \\nğŸŒŸ AI Trends 2024: From self-supervised learning & multimodal models to on-device/edge AI & foundation models turbocharging code, art & science. ğŸš€  \\nWhich trend will reshape your world? ğŸ‘‡  \\n#AI #MachineLearning #Tech\\n\\n2) Why it works  \\nâ€¢ Hook with emoji and year (â€œğŸŒŸ AI Trends 2024â€) grabs attention immediately.  \\nâ€¢ Lists 3â€“4 hot topics, each distinct (self-supervised, multimodal, edge AI, foundation models).  \\nâ€¢ Uses strong verbs (â€œturbochargingâ€) and concise descriptors.  \\nâ€¢ Ends with a question + pointing emoji (â€œğŸ‘‡â€) to drive replies.  \\nâ€¢ Three well-chosen hashtags cover both broad (#AI) and niche (#Tech) communities.  \\n\\n3) Opportunities for improvement  \\nâ€¢ Character count: 195/280â€”room to tag an influencer or add a link.  \\nâ€¢ Could drop one hashtag if you want to tag @someone important.  \\nâ€¢ Consider a very short URL to a blog or infographic for extra value.  \\nâ€¢ Tone: energetic but could be more playful (e.g., a wink ğŸ˜‰) or urgent (â€œdonâ€™t get left behind!â€).  \\n\\n4) Detailed recommendations  \\n- Length: Aim for 180â€“250 chars. Enough room for 2â€“3 hashtags, one @mention, and a tiny URL.  \\n- Hook: Start with a bold statement or emoji to stand out in a fast feed.  \\n- Content:  \\n  â€¢ Pick 3â€“5 sub-trendsâ€”too many dilutes focus.  \\n  â€¢ Use vivid verbs (â€œboosting,â€ â€œrevolutionizing,â€ â€œunlockingâ€).  \\n- Engagement tactic:  \\n  â€¢ Ask a direct question or poll (â€œWhich AI trend are you most excited about? Vote â¬‡ï¸â€).  \\n  â€¢ Invite tagging (â€œTag a coworker who needs to see this!â€).  \\n- Style & tone:  \\n  â€¢ Keep sentences shortâ€”each idea gets its own phrase.  \\n  â€¢ Mix in 1â€“2 emojis max for personality.  \\n  â€¢ Use active voice (â€œAI is eating the worldâ€ instead of â€œThe world is being eaten by AIâ€).  \\n- Virality boosters:  \\n  â€¢ Hashtags: 2â€“3 highly relevant ones.  \\n  â€¢ Mentions: @influencers or @OpenAI, @GoogleAI for retweet potential.  \\n  â€¢ Media: Attach a striking graphic or short clip summarizing the trends.  \\n\\n5) Next Steps  \\nâ€“ Draft your version using the formula above.  \\nâ€“ Keep it under 250 characters.  \\nâ€“ Add one mention or link.  \\nâ€“ Post during peak hours (9â€“11 am or 5â€“7 pm in your audienceâ€™s timezone).  \\nâ€“ Engage immediately with any replies to accelerate the algorithmâ€™s favor.  \\n\\nWith these tweaks, your AI-trends tweet will be clear, compelling and primed for retweets, likes and conversation. Good luck! ğŸš€', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1184, 'prompt_tokens': 60, 'total_tokens': 1244, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-ClcuLj2SapT0tGCWxFWtzZXNylv0y', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0e08-bff4-7552-aae2-8ca507161f15-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1184, 'total_tokens': 1244, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reflect_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"I want to write a twitter post about the latest trends in AI.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## ğŸ“ Summary\n",
    "\n",
    "Congratulations! ğŸ‰ You've learned:\n",
    "\n",
    "âœ… How to set up LangChain chat models  \n",
    "âœ… The difference between message roles  \n",
    "âœ… How to invoke models with strings and message objects  \n",
    "âœ… Understanding response objects and extracting content  \n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "- Learn about prompt templates\n",
    "- Build conversation chains\n",
    "- Explore memory and context\n",
    "- Create RAG agents\n",
    "\n",
    "### ğŸ“š Resources:\n",
    "\n",
    "- [LangChain Documentation](https://docs.langchain.com)\n",
    "- [LangChain Python Reference](https://reference.langchain.com/python)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding!** ğŸš€âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
