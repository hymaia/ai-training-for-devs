{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ü§ñ Using LLMs with LangChain - A Beginner's Guide\n",
    "\n",
    "Welcome to this interactive tutorial! In this notebook, you'll learn how to:\n",
    "- üîß Set up and configure chat models\n",
    "- üí¨ Interact with AI using different message types\n",
    "- üöÄ Make your first API calls to language models\n",
    "\n",
    "Let's dive in! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö What are Chat Models?\n",
    "\n",
    "Chat models are AI systems that:\n",
    "- Take a **sequence of messages** as input\n",
    "- Return **intelligent responses** as output\n",
    "\n",
    "Think of it like having a conversation with an AI assistant!\n",
    "\n",
    "### üåü Why LangChain?\n",
    "\n",
    "LangChain makes it easy to work with different AI models through a unified interface. In this course, we'll use:\n",
    "\n",
    "| Model | Why? | Link |\n",
    "|-------|------|------|\n",
    "| **ChatOpenAI** | Popular & performant | [Documentation](https://docs.langchain.com/oss/python/integrations/chat/openai) |\n",
    "\n",
    "> **üìù Note:** You'll need an `OPENAI_API_KEY` to follow along. Don't have one? Check out [OpenAI's platform](https://platform.openai.com/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## üîë Step 1: Environment Setup\n",
    "\n",
    "First, let's load our API keys from the environment. This keeps your secrets safe! üîí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-setup",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéõÔ∏è Step 2: Initialize the Chat Model\n",
    "\n",
    "Now we'll set up our AI model. We have two options:\n",
    "\n",
    "### Option A: Using OpenRouter üîÑ\n",
    "\n",
    "OpenRouter allows you to access multiple AI models through a single API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "openrouter-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Model initialized with OpenRouter!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Configure model with OpenRouter\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",  # Specify the model you want to use\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0,  # 0 = deterministic, 1 = creative\n",
    ")\n",
    "\n",
    "print(\"üéâ Model initialized with OpenRouter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-openai",
   "metadata": {},
   "source": [
    "### Option B: Direct OpenAI Connection üéØ\n",
    "\n",
    "Or connect directly to OpenAI's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-openai-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Model initialized with OpenAI!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Simple direct connection\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "print(\"üéâ Model initialized with OpenAI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temperature-explained",
   "metadata": {},
   "source": [
    "#### üå°Ô∏è Understanding Temperature\n",
    "\n",
    "The `temperature` parameter controls response creativity:\n",
    "\n",
    "```\n",
    "0.0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1.0\n",
    "‚îÇ                            ‚îÇ\n",
    "Predictable              Creative\n",
    "Consistent               Varied\n",
    "Focused                  Exploratory\n",
    "```\n",
    "\n",
    "- **temperature=0**: Best for factual answers, code, translations\n",
    "- **temperature=0.7**: Good balance for most tasks\n",
    "- **temperature=1**: Maximum creativity for stories, brainstorming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-interaction",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Your First AI Interaction!\n",
    "\n",
    "Let's send a message to the AI and see what happens! üéä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simple-invoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì© Full Response Object:\n",
      "==================================================\n",
      "content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_7abc656409', 'id': 'chatcmpl-ClR3Y0cnH3PUbeLB2pqKNAc2yp1m3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b0b51-9c81-7871-a689-dc276222920a-0' usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Sending a simple string message\n",
    "response = model.invoke(\"Hello, world!\")\n",
    "\n",
    "print(\"üì© Full Response Object:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "response-structure",
   "metadata": {},
   "source": [
    "### üîç Understanding the Response\n",
    "\n",
    "The response object contains:\n",
    "- `content`: The actual AI message\n",
    "- `response_metadata`: Token usage, costs, model info\n",
    "- `id`: Unique identifier for this interaction\n",
    "\n",
    "Most of the time, you'll just want the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extract-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ AI says:\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Extract just the message content\n",
    "print(\"üí¨ AI says:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "messages-explanation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí¨ Step 4: Working with Message Roles\n",
    "\n",
    "In real conversations, different participants have different roles. In AI chat, we typically have:\n",
    "\n",
    "| Role | Purpose | Example |\n",
    "|------|---------|------|\n",
    "| **System** üé≠ | Sets behavior/personality | \"You are a helpful coding assistant\" |\n",
    "| **User** üë§ | Your questions/inputs | \"What is the capital of France?\" |\n",
    "| **Assistant** ü§ñ | AI's responses | \"The capital of France is Paris.\" |\n",
    "\n",
    "![Message Flow](https://via.placeholder.com/700x200/9B59B6/FFFFFF?text=System+Sets+Rules+‚Üí+User+Asks+‚Üí+Assistant+Answers)\n",
    "\n",
    "### üéØ System Messages: Setting the Stage\n",
    "\n",
    "System messages are like giving the AI a role to play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message-roles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Geography Expert says:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Creating a structured conversation\n",
    "response = model.invoke(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful geography expert who loves to share interesting facts.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üåç Geography Expert says:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "message-types",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì® Step 5: Using Message Objects\n",
    "\n",
    "LangChain provides special message classes for cleaner code:\n",
    "\n",
    "```python\n",
    "HumanMessage    ‚Üí Messages from the user\n",
    "AIMessage       ‚Üí Responses from the AI  \n",
    "SystemMessage   ‚Üí System instructions\n",
    "```\n",
    "\n",
    "### üé® Benefits of Message Objects:\n",
    "- üìù More readable code\n",
    "- üè∑Ô∏è Can add metadata (like names)\n",
    "- üîÑ Easier to build conversation histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message-objects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì AI's response to student:\n",
      "Hello! That's awesome to hear! AI is a fascinating field with so much to explore. What would you like to learn about first? Machine learning, natural language processing, computer vision, or something else?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message with metadata\n",
    "msg = HumanMessage(\n",
    "    content=\"Hello world! I'm excited to learn about AI!\",\n",
    "    name=\"Student\",  # Optional: identify the speaker\n",
    ")\n",
    "\n",
    "# Send as a list\n",
    "messages = [msg]\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"üéì AI's response to student:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3607efc",
   "metadata": {},
   "source": [
    "The response contains multiple pieces of information:\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content='...',              # The actual message text\n",
    "    response_metadata={         # Metadata about the response\n",
    "        'token_usage': {...},   # How many tokens were used\n",
    "        'model_name': '...',    # Which model processed this\n",
    "        'finish_reason': '...'  # Why the response ended\n",
    "    },\n",
    "    id='...',                   # Unique identifier\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shorthand",
   "metadata": {},
   "source": [
    "### ‚ö° Quick Tip: Shorthand Method\n",
    "\n",
    "For simple queries, you can skip the message objects entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shorthand-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have access to real-time weather data. You can check the current weather by using a weather website or app like Weather.com, AccuWeather, or a voice assistant on your device. If you tell me your location, I can help guide you on where to find the forecast!\n"
     ]
    }
   ],
   "source": [
    "# This automatically converts to a HumanMessage\n",
    "response = model.invoke(\"What's the weather like?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-overview",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Step 6: Essential Methods\n",
    "\n",
    "LangChain chat models come with powerful methods:\n",
    "\n",
    " üìû `invoke()` - Single Response\n",
    "```python\n",
    "response = model.invoke(\"Your question\")\n",
    "# Returns complete response at once\n",
    "```\n",
    "\n",
    "**Best for:** Short answers, quick questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38778099",
   "metadata": {},
   "source": [
    "üåä `stream()` - Real-time Streaming\n",
    "```python\n",
    "for chunk in model.stream(\"Tell me a story\"):\n",
    "    print(chunk.content, end=\"\")\n",
    "# Shows response as it's generated\n",
    "```\n",
    "\n",
    "**Best for:** Long responses, better UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb3093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here‚Äôs a story for you:\n",
      "\n",
      "---\n",
      "\n",
      "**The Lantern of Whispering Woods**\n",
      "\n",
      "In a small village nestled at the edge of the Whispering Woods, there lived a curious girl named Elara. The villagers often spoke of the forest with a mix of fear and wonder, for it was said that the trees could whisper secrets to those who dared to listen.\n",
      "\n",
      "One evening, as the sun dipped below the horizon, Elara found an old, dusty lantern in her grandmother‚Äôs attic. It was unlike any lantern she had seen before‚Äîits glass glowed faintly with a soft, blue light, and intricate symbols were etched into its metal frame.\n",
      "\n",
      "Intrigued, Elara took the lantern and ventured into the Whispering Woods. As she walked deeper among the ancient trees, the whispers grew louder, forming words she could almost understand. The lantern‚Äôs light pulsed gently, guiding her path.\n",
      "\n",
      "Suddenly, she came upon a clearing where the air shimmered with magic. In the center stood a majestic tree, its bark glowing with the same blue light as the lantern. Elara held up the lantern, and the tree‚Äôs whispers became clear: it spoke of a hidden realm where lost dreams and forgotten hopes lived, waiting to be found.\n",
      "\n",
      "With a brave heart, Elara stepped through a shimmering veil beneath the tree and found herself in a world of vibrant colors and floating lights. Here, she met the Dreamkeeper, a gentle spirit who entrusted her with a mission: to carry the lantern back to her village and share its light, so that no dream would ever be lost again.\n",
      "\n",
      "Elara returned home, the lantern‚Äôs glow brightening the night. From that day on, the villagers no longer feared the Whispering Woods. Instead, they gathered around Elara‚Äôs lantern, listening to the whispers and remembering their dreams, knowing that magic was never far away.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to tell you another story?"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"Tell me a story\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb9fa2",
   "metadata": {},
   "source": [
    "Sometimes you might want to use the builtin rompttemplate and the chains of LangChains (But I don't really like them :p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e09f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30126f7",
   "metadata": {},
   "source": [
    "You can use tuple for messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6ca810",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a viral twitter influencer grading a tweet. Generate critique and recommendations for the user's tweet.\"\n",
    "            \"Always provide detailed recommendations, including requests for length, virality, style, etc.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe25257",
   "metadata": {},
   "source": [
    "Or you can use dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a twitter techie influencer assistant tasked with writing excellent twitter posts.\"\n",
    "            \" Generate the best twitter post possible for the user's request.\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want to write a twitter post about the latest trends in AI.\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Tutor's Response:\n",
      "You can reverse a string in Python using slicing. Here's a simple example:\n",
      "\n",
      "```python\n",
      "my_string = \"Hello, world!\"\n",
      "reversed_string = my_string[::-1]\n",
      "print(reversed_string)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "!dlrow ,olleH\n",
      "```\n",
      "\n",
      "The slice `[::-1]` means start from the end towards the first taking each character in reverse order. Let me know if you'd like other methods!\n"
     ]
    }
   ],
   "source": [
    "# Using message objects with metadata\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful coding tutor specializing in Python.\"),\n",
    "    HumanMessage(\n",
    "        content=\"How do I reverse a string in Python?\",\n",
    "        name=\"Student\",  # Optional: identify the speaker\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"üéì Tutor's Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87385634",
   "metadata": {},
   "source": [
    "### Building Conversation History üí≠\n",
    "\n",
    "For multi-turn conversations, include previous messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44471673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Math Tutor:\n",
      "Sure! Here's one way to calculate 15 * 24 step-by-step:\n",
      "\n",
      "1. Break 24 into 20 + 4.\n",
      "2. Multiply 15 by 20:  \n",
      "   15 * 20 = 300\n",
      "3. Multiply 15 by 4:  \n",
      "   15 * 4 = 60\n",
      "4. Add the two results together:  \n",
      "   300 + 60 = 360\n",
      "\n",
      "So, 15 * 24 = 360.\n"
     ]
    }
   ],
   "source": [
    "# Simulating a multi-turn conversation\n",
    "conversation = [\n",
    "    SystemMessage(content=\"You are a math tutor.\"),\n",
    "    HumanMessage(content=\"What is 15 * 24?\"),\n",
    "    AIMessage(content=\"15 * 24 = 360\"),\n",
    "    HumanMessage(content=\"Can you show me how you calculated that?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "\n",
    "print(\"üìö Math Tutor:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1f80b",
   "metadata": {},
   "source": [
    "\n",
    "## LangChain Expression Language (LCEL) üîó\n",
    "\n",
    "### What is LCEL? ü§î\n",
    "\n",
    "LCEL is LangChain's way of chaining components together using the `|` (pipe) operator.\n",
    "\n",
    "```python\n",
    "# Traditional approach\n",
    "formatted = prompt.format(...)\n",
    "response = model.invoke(formatted)\n",
    "\n",
    "# LCEL approach ‚ú®\n",
    "chain = prompt | model\n",
    "response = chain.invoke({...})\n",
    "```\n",
    "\n",
    "### Why Use LCEL? üåü\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Composability** | Chain multiple operations easily |\n",
    "| **Streaming** | Automatic streaming support |\n",
    "| **Async** | Built-in async execution |\n",
    "| **Batching** | Process multiple inputs efficiently |\n",
    "| **Debugging** | Better error messages and tracing |\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "Input ‚îÄ‚îÄ> Prompt ‚îÄ‚îÄ> Model ‚îÄ‚îÄ> Output Parser ‚îÄ‚îÄ> Final Result\n",
    "          Template\n",
    "          \n",
    "With LCEL:\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ebbd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"o4-mini\")\n",
    "\n",
    "generate_chain = generation_prompt | llm\n",
    "reflect_chain = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429529aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='üî• AI Trends to Watch in 2024 üî•\\n\\n‚Ä¢ Generative AI (#GPT-4, #StableDiffusion) ignites creativity  \\n‚Ä¢ Multimodal models bridge vision + language  \\n‚Ä¢ Self-supervised learning reduces label hunger  \\n‚Ä¢ Edge AI & tinyML power smarter IoT  \\n‚Ä¢ Responsible AI & ethics front and center  \\n\\n#AI #MachineLearning #DeepLearning', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 480, 'prompt_tokens': 66, 'total_tokens': 546, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-ClRAuDmcDvPrYDCbBlgAA9xblqOM4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0b58-8d66-7991-a6b1-56b52b1db585-0', usage_metadata={'input_tokens': 66, 'output_tokens': 480, 'total_tokens': 546, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"I want to write a twitter post about the latest trends in AI.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccad160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure‚ÄîI can help you craft a tweet that grabs attention and drives engagement around AI‚Äôs newest trends. To get started:\\n\\n1. Ask for your draft (so I can critique it)  \\n2. Or follow these guidelines and sample template to build your own:\\n\\n‚Äì‚Äì Tweet-Writing Best Practices ‚Äì‚Äì  \\n‚Ä¢ Hook in first 50 characters: pose a provocative question or share a bold stat.  \\n‚Ä¢ Keep it under 240 characters so it‚Äôs easy to RT and quote-tweet.  \\n‚Ä¢ Use 1‚Äì2 high-impact hashtags (#AI #AITrends #MachineLearning).  \\n‚Ä¢ Tag a key influencer or org if you‚Äôre referencing their work.  \\n‚Ä¢ Add an emoji or two for visual pop, but don‚Äôt overdo it.  \\n‚Ä¢ Ask a follow-up question or invite opinions to boost replies.  \\n\\n‚Äì‚Äì Sample Tweet Draft ‚Äì‚Äì  \\n‚ÄúIs AI about to reinvent creativity? üé®  \\nFrom GPT-4‚Äôs multimodal art generation to TinyML on your phone, 2024‚Äôs AI trends are breaking all the rules.  \\nWhich innovation will disrupt your industry next? üëá  \\n#AI #AITrends‚Äù\\n\\nCharacter count: ~200  \\nWhy it works:  \\n‚Ä¢ Opens with a question + emoji hook  \\n‚Ä¢ Mentions two hot topics (multimodal models, TinyML)  \\n‚Ä¢ Ends with a call-to-action asking for replies  \\n‚Ä¢ Keeps hashtags tight (2)  \\n\\nNext steps:  \\n1. Adjust the topics to your specific focus (e.g. explainable AI, AI governance, agentic systems).  \\n2. Swap in a jaw-dropping stat or link to a 1-line breakdown (e.g. ‚ÄúGPT-4 vision can now‚Ä¶‚Äù).  \\n3. Share your own draft here, and I‚Äôll give you line-by-line feedback on tone, word choice, hashtag strategy and ideal length.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 914, 'prompt_tokens': 60, 'total_tokens': 974, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-ClRAzsMA9JD8huQ2TAtFE9j2OFmGQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0b58-a440-7391-8efa-fa7a4cca4955-0', usage_metadata={'input_tokens': 60, 'output_tokens': 914, 'total_tokens': 974, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reflect_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"I want to write a twitter post about the latest trends in AI.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "Congratulations! üéâ You've learned:\n",
    "\n",
    "‚úÖ How to set up LangChain chat models  \n",
    "‚úÖ The difference between message roles  \n",
    "‚úÖ How to invoke models with strings and message objects  \n",
    "‚úÖ Understanding response objects and extracting content  \n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- Learn about prompt templates\n",
    "- Build conversation chains\n",
    "- Explore memory and context\n",
    "- Create RAG agents\n",
    "\n",
    "### üìö Resources:\n",
    "\n",
    "- [LangChain Documentation](https://docs.langchain.com)\n",
    "- [LangChain Python Reference](https://reference.langchain.com/python)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding!** üöÄ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
