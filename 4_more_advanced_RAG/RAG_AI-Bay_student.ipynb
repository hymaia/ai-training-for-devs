{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ RAG AI-Bay - Building an AI-Powered Customer Care Service\n",
    "\n",
    "## üéØ Welcome to the Advanced RAG Challenge!\n",
    "\n",
    "In this hands-on project, you'll solve a real-world problem: How can customers get instant, accurate answers to their questions about a second-hand marketplace? You'll build a **Retrieval-Augmented Generation (RAG)** system that does exactly that.\n",
    "The dataset is very close to what we could have in reality!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÑ Step 2: Loading the Customer Care FAQ Data\n",
    "\n",
    "We'll work with a JSON file containing frequently asked questions about the Olympics.\n",
    "\n",
    "### üìä Data Structure:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"faq_id\": \"21561426636690\",\n",
    "    \"faq_title\": \"**Title:** Professional...\",\n",
    "    \"faq_body\": \"**The Steps for Sellin..\",\n",
    "    \"updated_at\": \"2024-11-18T14:28\",\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "Let's load and explore the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAQ data\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_dir = Path.cwd()\n",
    "data_path = notebook_dir / \"data\" / \"faq_en.json\"\n",
    "with open(data_path, \"r\") as f:\n",
    "    faq_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(faq_data)} FAQ entries!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first entry\n",
    "faq_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample entries\n",
    "print(\"üìã Sample FAQ Entries:\\n\")\n",
    "for i, entry in enumerate(faq_data[:3], 1):\n",
    "    print(f\"{i}. {entry['faq_title']}\")\n",
    "    print(f\"{entry['faq_body'][:100]}...\\n\")\n",
    "\n",
    "print(f\"üí° Total entries: {len(faq_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ These documents are significantly longer than the previous exercice, aren't they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: In your opinion, what kind of preprocessing should we add in comparison with the previous exercice, and why ? (you could list at least 3 extra steps!)\n",
    "\n",
    "Click [here](https://docs.langchain.com/oss/python/integrations/splitters) for a hint (if you have really no idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your answer here:\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Exercise 1: Using what you learned in the previous exercise, build an ingestion pipeline that processes raw text all the way to the vector store. Keep in mind you'll need to add some extra steps (which are crucial!) to handle these longer documents effectively. They're not super long though, so don't worry! üòä\n",
    "\n",
    "try those queries to self-evaluate your rag:\n",
    "- query = \"Why can't I find my ad ?\"\n",
    "\n",
    "- query=\"On the AI-Bay website, I don‚Äôt have the option to add an IBAN or a payment card.  \\nI went to Settings ‚Üí Payment Methods, but there‚Äôs no way to make any changes.\"\n",
    "\n",
    "- query = \"Hello,  \\n\\nThank you in advance for permanently DELETING my AI-Bay account, as it has been ‚Äúblocked‚Äù and I can no longer use it.  \\n\\nKind regards.\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GO GO GO ! Code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Exercise 2: now try to add a hybrid search to the pipeline. \n",
    "[hint here](https://docs.langchain.com/oss/python/integrations/vectorstores/qdrant)\n",
    "\n",
    "Are the results better ? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GO GO GO ! Code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Exercise 3**: Create a `RagConversation` class that handles:\n",
    "- Document retrieval from your vector store\n",
    "- Response generation using the LLM\n",
    "- Conversation history management\n",
    "\n",
    "See the message history implementation example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "memory = []\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing well, thank you!\"),\n",
    "]\n",
    "\n",
    "memory.extend(messages)\n",
    "\n",
    "for message in memory:\n",
    "    # message.pretty_print()\n",
    "    print(f\"{message.type}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GO GO GO ! Code here.\n",
    "\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "class RagConversation:\n",
    "    def __init__(self, vector_store, llm):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "        self.history = []\n",
    "\n",
    "    def add_message(self, message: BaseMessage):\n",
    "        self.history.append(message)\n",
    "\n",
    "    def history_to_string(self):\n",
    "        return \"\\n\".join(\n",
    "            [f\"{message.type}: {message.content}\" for message in self.history]\n",
    "        )\n",
    "\n",
    "    def get_response(self, question):\n",
    "        # TODO: code this function\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GOGOOG !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it with the gradio UI !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "K = 5\n",
    "\n",
    "rag_conversation = RagConversation(vector_store, llm)\n",
    "\n",
    "\n",
    "def rag_assistant_response(message, history):\n",
    "    return rag_conversation.get_response(message).content\n",
    "\n",
    "\n",
    "gr.ChatInterface(fn=rag_assistant_response).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "### üîß Immediate Improvements:\n",
    "1. Add more FAQ entries to the database\n",
    "2. Experiment with different chunk sizes\n",
    "3. Try different embedding models\n",
    "4. Adjust retrieval count (k parameter)\n",
    "5. Fine-tune the prompt template\n",
    "\n",
    "### üéì Advanced Topics to Explore:\n",
    "1. **Hybrid Search:** Combine keyword + semantic\n",
    "2. **Reranking:** Score and reorder retrieved docs\n",
    "3. **Query Expansion:** Generate multiple query variations\n",
    "4. **Metadata Filtering:** Filter by date, source, etc.\n",
    "5. **Multi-modal RAG:** Include images, tables\n",
    "6. **Streaming Responses:** Real-time token generation\n",
    "7. **Conversation Memory:** Multi-turn conversations\n",
    "8. **Evaluation Metrics:** Measure RAG quality\n",
    "\n",
    "### üåü Project Ideas:\n",
    "1. Build a RAG system for your company docs\n",
    "2. Create a research paper Q&A system\n",
    "3. Make a personal knowledge base assistant\n",
    "4. Build a code documentation helper\n",
    "5. Create a study buddy for textbooks\n",
    "\n",
    "---\n",
    "\n",
    "## üí≠ Final Thoughts\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 25px; border-radius: 15px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin-top: 0;\">üéì Remember:</h3>\n",
    "    <p><strong>\"The best RAG system is the one that solves your specific problem.\"</strong></p>\n",
    "    <ul>\n",
    "        <li>Start simple and iterate</li>\n",
    "        <li>Measure what matters to your users</li>\n",
    "        <li>Focus on retrieval quality first</li>\n",
    "        <li>Then optimize generation</li>\n",
    "        <li>Always test with real users</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center; background: linear-gradient(135deg, #FF6B6B 0%, #FFE66D 100%); padding: 30px; border-radius: 15px; margin: 30px 0;\">\n",
    "    <h2 style=\"margin: 0; font-size: 2.5em;\">üèÖ You're Now a RAG Expert!</h2>\n",
    "    <p style=\"margin: 20px 0 0 0; font-size: 1.3em; font-weight: bold;\">Go forth and build amazing AI-powered systems! üöÄ</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [RAG Best Practices](https://www.anthropic.com/research)\n",
    "\n",
    "### ü§ù Community:\n",
    "\n",
    "- LangChain Discord\n",
    "- r/MachineLearning\n",
    "- AI Stack Exchange\n",
    "- Hugging Face Forums\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Building! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
